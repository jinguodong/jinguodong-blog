<?xml version="1.0" encoding="utf-8"?>
  <rss version="2.0"
        xmlns:content="http://purl.org/rss/1.0/modules/content/"
        xmlns:atom="http://www.w3.org/2005/Atom"
  >
  <channel>
    <title>guodong.j</title>
    <link href="/jinguodong-blog/feed/" rel="self" />
    <link href="http://geeklu.com" />
    <lastBuildDate>2015-04-17T18:15:39+08:00</lastBuildDate>
    <webMaster>buptjinguodong@gmail.com</webMaster>
    
    <item>
      <title>Python核心编程（第二版）</title>
      <link href="/jinguodong-blog/2015/04/core-python-programming/"/>
      <pubDate>2015-04-17T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/core-python-programming</guid>
      <content:encoded><![CDATA[<h2>对象</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>自己动手写网络爬虫</title>
      <link href="/jinguodong-blog/2015/04/self-spider/"/>
      <pubDate>2015-04-15T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/self-spider</guid>
      <content:encoded><![CDATA[<h2>需要解决的问题</h2>

<h2>获取海量数据</h2>

<p>WebNews Crawler是通过HTTP下载资源的Java爬虫。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Heritrix网络爬虫</title>
      <link href="/jinguodong-blog/2015/04/heritrix-spider/"/>
      <pubDate>2015-04-15T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/heritrix-spider</guid>
      <content:encoded><![CDATA[<h2>前期调研</h2>

<p><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-heritrix/">利用 Heritrix 构建特定站点爬虫</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Web Spider With Python</title>
      <link href="/jinguodong-blog/2015/04/web-spider-with-python/"/>
      <pubDate>2015-04-14T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/web-spider-with-python</guid>
      <content:encoded><![CDATA[<h2>前期调研</h2>

<p><a href="http://www.nowamagic.net/academy/detail/1302879"></a>
<a href="http://blog.csdn.net/sssogs/article/details/8574595">Python 之 BNUOJ代码抓取器</a>
<a href="http://www.isaced.com/post-197.html"></a>
<a href="http://bbs.chinaunix.net/thread-3779115-1-1.html"></a></p>

<h2>异常处理和HTTP状态码分类</h2>

<ol>
<li>URLError</li>
<li>HTTPError</li>
</ol>


<p><strong>HTTP状态码</strong></p>

<pre><code>200：请求成功      处理方式：获得响应的内容，进行处理 
201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到    处理方式：爬虫中不会遇到 
202：请求被接受，但处理尚未完成    处理方式：阻塞等待 
204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。    处理方式：丢弃
300：该状态码不被HTTP/1.0的应用程序直接使用， 只是作为3XX类型回应的默认解释。存在多个可用的被请求资源。    处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃
301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源    处理方式：重定向到分配的URL
302：请求到的资源在一个不同的URL处临时保存     处理方式：重定向到临时的URL
304 请求的资源未更新     处理方式：丢弃 
400 非法请求     处理方式：丢弃 
401 未授权     处理方式：丢弃 
403 禁止     处理方式：丢弃 
404 没有找到     处理方式：丢弃 
5XX 回应代码以“5”开头的状态码表示服务器端发现自己出现错误，不能继续执行请求    处理方式：丢弃
</code></pre>

<p><strong>异常处理经典案例</strong></p>

<pre><code>from urllib2 import Request, urlopen, URLError, HTTPError

req = Request('http://bbs.csdn.net/callmewhy')
try:
    response = urlopen(req)
except HTTPError, e:
    print 'The server couldn\'t fulfill the request.'
    print 'Error code: ', e.code
except URLError, e:
    print 'We failed to reach a server.'
    print 'Reason: ', e.reason
else:
    print 'No exception was raised.'
    # everything is fine
</code></pre>

<h2>Opener与Handler</h2>

<p>...</p>

<h2>urllib2的使用细节与抓站技巧</h2>

<p>使用说明：</p>

<p><strong>1 proxy的设置</strong></p>

<p>urllib2 默认会使用环境变量 <strong>http_proxy</strong> 来设置 HTTP Proxy。如果想在程序中明确控制 Proxy，而不受环境变量的影响，可以使用下面的方式</p>

<pre><code>import urllib2

enable_proxy = True
proxy_handler = urllib2.ProxyHandler({"http" : 'http://some-proxy.com:8080'})
null_proxy_handler = urllib2.ProxyHandler({})

if enable_proxy:
    opener = urllib2.build_opener(proxy_handler)
else:
    opener = urllib2.build_opener(null_proxy_handler)

urllib2.install_opener(opener)
</code></pre>

<p>这里要注意的一个细节，使用 <strong>urllib2.install_opener()</strong> 会设置 urllib2 的全局 opener。这样后面的使用会很方便，但不能做更细粒度的控制，比如想在程序中使用两个不同的 Proxy 设置等。比较好的做法是不使用 install_opener 去更改全局的设置，而只是直接调用 opener 的 open 方法代替全局的 urlopen 方法。</p>

<p><strong>2 Timeout 设置</strong></p>

<p>在老版本中，urllib2 的 API 并没有暴露 Timeout 的设置，要设置 Timeout 值，只能更改 Socket 的全局 Timeout 值。</p>

<p>import urllib2
import socket</p>

<p>socket.setdefaulttimeout(10) # 10 秒钟后超时
urllib2.socket.setdefaulttimeout(10) # 另一种方式
在新的 Python 2.6 版本中，超时可以通过 urllib2.urlopen() 的 timeout 参数直接设置。</p>

<p>import urllib2
response = urllib2.urlopen('http://www.google.com', timeout=10)</p>

<p><strong>3 在 HTTP Request 中加入特定的 Header</strong></p>

<p>要加入 Header，需要使用 Request 对象：</p>

<p>import urllib2</p>

<p>request = urllib2.Request(uri)
request.add_header('User-Agent', 'fake-client')
response = urllib2.urlopen(request)
对有些 header 要特别留意，Server 端会针对这些 header 做检查</p>

<p>User-Agent 有些 Server 或 Proxy 会检查该值，用来判断是否是浏览器发起的 Request
Content-Type 在使用 REST 接口时，Server 会检查该值，用来确定 HTTP Body 中的内容该怎样解析。</p>

<p>常见的取值有：</p>

<p>application/xml ：在 XML RPC，如 RESTful/SOAP 调用时使用
application/json ：在 JSON RPC 调用时使用
application/x-www-form-urlencoded ：浏览器提交 Web 表单时使用</p>

<p>……</p>

<p>在使用 RPC 调用 Server 提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致 Server 拒绝服务。</p>

<p><strong>4 Redirect</strong></p>

<p>urllib2 默认情况下会针对 3xx HTTP 返回码自动进行 Redirect 动作，无需人工配置。要检测是否发生了 Redirect 动作，只要检查一下 Response 的 URL 和 Request 的 URL 是否一致就可以了。</p>

<p>import urllib2
response = urllib2.urlopen('http://www.google.cn')
redirected = response.geturl() == 'http://www.google.cn'
如果不想自动 Redirect，除了使用更低层次的 httplib 库之外，还可以使用自定义的 HTTPRedirectHandler 类。</p>

<p>import urllib2</p>

<p>class RedirectHandler(urllib2.HTTPRedirectHandler):
    def http_error_301(self, req, fp, code, msg, headers):
        pass
    def http_error_302(self, req, fp, code, msg, headers):
        pass</p>

<p>opener = urllib2.build_opener(RedirectHandler)
opener.open('http://www.google.cn')</p>

<p><strong>5 Cookie</strong></p>

<p>urllib2 对 Cookie 的处理也是自动的。如果需要得到某个 Cookie 项的值，可以这么做：</p>

<p>import urllib2
import cookielib</p>

<p>cookie = cookielib.CookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))
response = opener.open('http://www.google.com')
for item in cookie:
    if item.name == 'some_cookie_item_name':
        print item.value</p>

<p><strong>6 使用 HTTP 的 PUT 和 DELETE 方法</strong>
urllib2 只支持 HTTP 的 GET 和 POST 方法，如果要使用 HTTP PUT 和 DELETE，只能使用比较低层的 httplib 库。虽然如此，我们还是能通过下面的方式，使 urllib2 能够发出 HTTP PUT 或 DELETE 的包：</p>

<p>import urllib2</p>

<p>request = urllib2.Request(uri, data=data)
request.get_method = lambda: 'PUT' # or 'DELETE'
response = urllib2.urlopen(request)
这种做法虽然属于 Hack 的方式，但实际使用起来也没什么问题。</p>

<p><strong>7 得到 HTTP 的返回码</strong></p>

<p>对于 200 OK 来说，只要使用 urlopen 返回的 response 对象的 getcode() 方法就可以得到 HTTP 的返回码。但对其它返回码来说，urlopen 会抛出异常。这时候，就要检查异常对象的 code 属性了：</p>

<p>import urllib2
try:
    response = urllib2.urlopen('http://restrict.web.com')
except urllib2.HTTPError, e:
    print e.code</p>

<p><strong>8 Debug Log</strong></p>

<p>使用 urllib2 时，可以通过下面的方法把 Debug Log 打开，这样收发包的内容就会在屏幕上打印出来，方便我们调试，在一定程度上可以省去抓包的工作。</p>

<p>import urllib2</p>

<p>httpHandler = urllib2.HTTPHandler(debuglevel=1)
httpsHandler = urllib2.HTTPSHandler(debuglevel=1)
opener = urllib2.build_opener(httpHandler, httpsHandler)</p>

<p>urllib2.install_opener(opener)
response = urllib2.urlopen('http://www.google.com')</p>

<p><strong>9.表单的处理</strong></p>

<p>登录必要填表，表单怎么填？
首先利用工具截取所要填表的内容。
比如我一般用firefox+httpfox插件来看看自己到底发送了些什么包。
以verycd为例，先找到自己发的POST请求，以及POST表单项。
可以看到verycd的话需要填username,password,continueURI,fk,login_submit这几项，其中fk是随机生成的（其实不太随机，看上去像是把epoch时间经过简单的编码生成的），需要从网页获取，也就是说得先访问一次网页，用正则表达式等工具截取返回数据中的fk项。continueURI顾名思义可以随便写，login_submit是固定的，这从源码可以看出。还有username，password那就很显然了：
[python] view plaincopy</p>

<h1>-<em>- coding: utf-8 -</em>-</h1>

<p>import urllib<br/>
import urllib2<br/>
postdata=urllib.urlencode({<br/>
    'username':'汪小光',<br/>
    'password':'why888',<br/>
    'continueURI':'http://www.verycd.com/',<br/>
    'fk':'',<br/>
    'login_submit':'登录'<br/>
})<br/>
req = urllib2.Request(<br/>
    url = 'http://secure.verycd.com/signin',<br/>
    data = postdata<br/>
)<br/>
result = urllib2.urlopen(req)<br/>
print result.read()</p>

<p><strong>10.伪装成浏览器访问</strong></p>

<p>某些网站反感爬虫的到访，于是对爬虫一律拒绝请求
这时候我们需要伪装成浏览器，这可以通过修改http包中的header来实现
[python] view plaincopy</p>

<h1>…</h1>

<p>headers = {<br/>
    'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'<br/>
}<br/>
req = urllib2.Request(<br/>
    url = 'http://secure.verycd.com/signin/*/http://www.verycd.com/',<br/>
    data = postdata,<br/>
    headers = headers<br/>
)</p>

<h1>...</h1>

<p><strong>11.对付"反盗链"</strong></p>

<p>某些站点有所谓的反盗链设置，其实说穿了很简单，
就是检查你发送请求的header里面，referer站点是不是他自己，
所以我们只需要像把headers的referer改成该网站即可，以cnbeta为例：</p>

<h1>...</h1>

<p>headers = {
    'Referer':'http://www.cnbeta.com/articles'
}</p>

<h1>...</h1>

<p>headers是一个dict数据结构，你可以放入任何想要的header，来做一些伪装。
例如，有些网站喜欢读取header中的X-Forwarded-For来看看人家的真实IP，可以直接把X-Forwarde-For改了。</p>

<h2>一个简单的百度贴吧的小爬虫</h2>

<p>...</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Python基础教程</title>
      <link href="/jinguodong-blog/2015/04/Basic-Python/"/>
      <pubDate>2015-04-14T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/Basic-Python</guid>
      <content:encoded><![CDATA[<h2>循环（条件、循环和其他辅助语句）</h2>

<p><strong>数据结构</strong>：序列(列表、元组)和字典</p>

<p><strong>循环方法</strong>：while、for</p>

<h3>序列:循环分片操作</h3>

<pre><code>numbers[begin, end, step]
numbers[0:10:1]
</code></pre>

<p>基本列表操作：</p>

<pre><code>list('Hello')
del numbers[2]
numbers.append(9) #add element 9 to the end of numbers
numbers.extend(9) # 通过分片赋值实现
numbers.index(5) #找到列表中第一个匹配项，若未找到抛出异常
numbers.insert(3, 'who') #在numbers[3]的位置上添加'who'元素
numbers.pop(空或number) #默认是最后一个元素，否则是指定位置元素
numbers.remove(4) # 删除第一个匹配项，不存在抛出异常    
numbers.reverse()
numbers.sort(空|自定义比较函数cmp|key=somefunc|reverse=TrueorFalse)
    x = [4,2,1,3]
    y = x[:] # 将x的所有元素的分片赋给y
    y.sort()
numbers.sorted()
</code></pre>

<p>元组：</p>

<pre><code>tuple([1,2,3])
</code></pre>

<h3>字典</h3>

<pre><code>dict([('name','miky'), ('age',42)])
d = dict(name='Mike', age=42)
</code></pre>

<p>字典基本操作：</p>

<pre><code>len(d)
del d[k] # 删除d中键为k的项
k in d # 判断d中是否含键为k的项
</code></pre>

<p>字典的格式化字符串</p>

<pre><code>phonebook = {"Cecil" : '9120'}
"Cecil's phone is %(Cecil)s" % phonebook
</code></pre>

<p>字典方法</p>

<pre><code>clear()
copy() # 浅复制
deepcopy() # 深复制函数
    from copy import deepcopy()
    dc = deepcopy(d)
fromkeys()
    dict.fromkeys(['name', 'age'], 空or自定义默认值)
get(key, 空or自定义默认值)
has_key()
items(), iteritems()
keys(), iterkeys()
values(), itervalues()
pop(key) # 返回可以指定的值，并移除key
popitem()
</code></pre>

<h3>条件执行语句</h3>

<pre><code>if num&gt;0:
    statement
elif num&lt;0:
    statement
else:
    statement
</code></pre>

<h3>断言</h3>

<pre><code>assert 0&lt;age&lt;10 #断言必须为真
</code></pre>

<h3>while循环 and for循环</h3>

<pre><code>words = ['this', 'is', 'an', 'ex']
for word in words:
    statement
</code></pre>

<p>遍历字典</p>

<pre><code>d = {'x':1, 'y':2, 'z':3}
for key in d:
    print key, 'correspondsto', d[key]
for key, value in d.items():
    statement
</code></pre>

<p>列表推算式</p>

<pre><code>[x*x for x in range(10) if x % 3 == 0]
</code></pre>

<p>三人行</p>

<pre><code>pass
del
exec and eval

scope={}
scope['x'] = 2
scope['y'] = 3
eval("x+y", scope)

scope={}
exec "x=1" in scope # 在scope域内执行x=1
eval("x*x", scope) # 在scope域内运算x*x
</code></pre>

<h2>异常</h2>

<pre><code>raise Exception
class SomeCustomException(Exception): pass
</code></pre>

<p><strong>捕捉异常</strong></p>

<pre><code>try:
    x/y
except ZeroDivisionError:
    print ...
except TypeError, e: # 得到异常对象e
    print ...
except Exception, e: # 全捕捉
    print ...

try:
    x/y
except (ZeroDivisionError, TypeError[, Others]):
    print ...
else: # 没有异常发生时被执行
    print ...
finally: # 绝对会被执行
    print ...
</code></pre>

<p><strong>异常的经典应用</strong></p>

<pre><code>def describePerson(person):
    print "Description of", person['name']
    print 'Age', person['age']
    try:
        print 'Occupation', person['occupation']
    except KeyError: pass

try:
    obj.write
except AttributeError:
    print ...
else:
    print ...
</code></pre>

<h2>抽象</h2>

<pre><code>class Filter:
    def init(self):
        self.blocked = []
    def filter(self, sequence):
        return [x for x in sequence if x not in self.blocked]
class SPAMFilter(Filter):
    def init(self):
        self.blocked = ['SPAM']

issubclass(SPAMFilter, Filter)
</code></pre>

<h2>文件和素材</h2>

<pre><code>open(name[, mode[, buffering]])
mode: r, w, a, b, +
buffering: 0, +num, -num
buffering控制着文件的缓冲，+num表示使用num字节大小的内存区作为缓冲，只有当使用flush和close时才会写入硬盘。
</code></pre>

<h2>Python和万维网</h2>

<p>1 屏幕抓取</p>

<pre><code>Tidy+HTMLParser
from subprocess import Popen, PIPE
text = open('messy.html').read()
tidy = Popen('tidy', stdin=PIPE, stdout=PIPE, stderr=PIPE)
tidy.stdin.write(text)
tidy.stdin.close()
print tidy.stdout.read() # 得到标准的XHTML格式的文档

HTMLParser:
handle_starttag(tag, attrs) # 找到开始标签时调用。attrs是（名称，值）对的序列
handle_data(data) # 使用文本数据时调用
handle_endtag(tag) # 找到标签结束时调用
一个经典的案例
from urllib import urlopen
from HTMLParser import HTMLParser

class Scraper(HTMLParser):
    in_h3 = False
    in_link = False

    def handle_starttag(self, tag, attrs):
        attrs = dict(attrs)
        if tag = 'h3':
            self.in_h3 = True
        if tag = 'a' and 'href' in attrs:
            self.in_link = True
            self.chunks = []
            self.url = attrs['href']
    def handle_data(self, data):
        if self.in_link:
            self.chunks.append(data)
    def handle_endtag(self, tag):
        if tag = 'h3':
            self.in_h3 = False
        if tag = 'a':
            if self.in_h3 and self.in_link:
                print '%s (%s)' % (''.join(self.chunks), self.url)
            self.in_link = False
text = urlopen('http://python.org/community/jobs').read()
parser = Scraper()
parser = feed(text)
parser = close()


"Beautiful Soup" FOR HTML
"Scrape 'N' Feed" FOR RSS feed
</code></pre>

<p><strong>Beautiful Soup:</strong></p>

<pre><code>Docs: http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html
四种对象:
    1. Tag
    2. NavigableString
    3. BeautifulSoup
    4. Comment

Tag:
tag.name
tag.attrs
最常见的多值的属性是 class (一个tag可以有多个CSS的class). 还有一些属性 rel , rev , accept-charset , headers , accesskey . 在Beautiful Soup中多值属性的返回类型是list

tag.string
字符串常被包含在tag内
</code></pre>

<p>遍历文档树</p>

<pre><code>获取&lt;body&gt;标签中的第一个&lt;b&gt;标签:
soup.body.b
得到所有的&lt;a&gt;标签：
soup.find_all('a')
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>CI框架学习笔记</title>
      <link href="/jinguodong-blog/2015/04/plan-list/"/>
      <pubDate>2015-04-09T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/plan-list</guid>
      <content:encoded><![CDATA[<h2>长目标（2015-4-9）</h2>

<ol>
<li>研究phalcon框架，做到能够完整的二次开发</li>
<li>研究爬虫技术</li>
<li>深究前段开发技术（bootstrap）</li>
<li>学习Java，使用J2EE技术开发一个网站</li>
<li>报考导游证</li>
<li>学习微信公众平台开发模式</li>
</ol>


<blockquote><p>要求做到如下要求：微信平台开发
要求：
熟悉微信公共平台开发模式，
对开发接口有了解，
可独立完成小的能组件，如自定义菜单，用户消息导出，分析等。</p></blockquote>

<ol>
<li>学习摄影技术（包括摄影前期，中期和后期处理）</li>
</ol>


<blockquote><p>http://www.zhihu.com/question/27004700/answer/34955464?utm_campaign=weekly168&amp;utm_source=weekly-digest&amp;utm_medium=email</p></blockquote>

<h2>爬虫技术研究计划表</h2>

<h3>前期调研</h3>

<p><a href="http://blog.csdn.net/column/details/why-bug.html">Python爬虫入门教程-请叫我汪海</a></p>

<p><a href="http://www.zhihu.com/question/20899988">如何入门 Python 爬虫-知乎</a></p>

<p><a href="http://www.zhihu.com/question/27621722">能利用爬虫技术做到哪些很酷很有趣很有用的事情？-知乎</a></p>

<p><a href="http://plough-man.com/?p=379">我的第一只爬虫：爬取豆瓣读书-tz二木</a></p>

<pre><code>之前同学有用过带界面的程序，貌似是专门爬某门户网站新闻用的。但是没用多久就跟我说想爬淘宝的商品信息，怎么办，我告诉他自己写一个，然后就没有然后了。爬虫这种东西针对性很强，每个网站的就够，数据获取方式，数据结构千变万化，所以自己写个没什么坏处（如果你是个程序员或者经常从事相关的数据搜集工作）。

可以从java入手，很容易找到中文教程。
httpClient+jsoup
先试着爬一个不用登陆，安全级别低的网站，例如成人网站（现在电脑里还有某成人网站所有电影的磁力连接。。）
然后爬一陆个需要登的网站，你会就会去研究模拟登陆，http协议，cookie这些东西
然后再爬一个难度较高的网站，新浪微博啦，豆瓣啦，你就会研究识别验证码，反爬虫机制之类的东西
如果你想把爬下来的东西做成像百度，谷歌一样的搜索引擎，可以研究lucene，solr这些东西
如果你想提高你的爬虫效率，你就会研究多线程，分布式这些东西，搞个mongoDB玩玩也是不错的

这些都是我今年年初做毕业论文的时候学到的，总之东西都不难，难的是真正动手去做，越做越有自信。
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>CI框架学习笔记</title>
      <link href="/jinguodong-blog/2015/04/CI-Study/"/>
      <pubDate>2015-04-09T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/04/CI-Study</guid>
      <content:encoded><![CDATA[
]]></content:encoded>
    </item>
    
    <item>
      <title>iscroll-5学习笔记</title>
      <link href="/jinguodong-blog/2015/03/front-note/"/>
      <pubDate>2015-03-29T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/03/front-note</guid>
      <content:encoded><![CDATA[<h2>HTML内所有元素的默认display形态</h2>

<pre><code>.dcss html,.dcss address,.dcss blockquote,.dcss body,.dcss dd,.dcss div,
.dcss dl,.dcss dt,.dcss fieldset,.dcss form,.dcss frame,.dcss frameset,
.dcss h1,.dcss h2,.dcss h3,.dcss h4,.dcss h5,.dcss h6,.dcss noframes,
.dcss ol,.dcss p,.dcss ul,.dcss center,.dcss dir,.dcss hr,.dcss menu, 
.dcss pre { display: block }
.dcss li { display: list-item }
.dcss head { display: none }
.dcss table { display: table }
.dcss tr { display: table-row }
.dcss thead { display: table-header-group }
.dcss tbody { display: table-row-group }
.dcss tfoot { display: table-footer-group }
.dcss col { display: table-column }
.dcss colgroup { display: table-column-group }
.dcss td,.dcss  th { display: table-cell; }
.dcss caption { display: table-caption }
.dcss th { font-weight: bolder; text-align: center }
.dcss caption { text-align: center }
.dcss body { margin: 8px; line-height: 1.12 }
.dcss h1 { font-size: 2em; margin: .67em 0;line-height: 1.5em }
.dcss h2 { font-size: 1.5em; margin: .75em 0;line-height: 1.5em }
.dcss h3 { font-size: 1.17em; margin: .83em 0;line-height: 1.5em }
.dcss h4 { font-size: 1.09em; margin: 1.12em 0;line-height: 1.5em }
.dcss h4,.dcss p,.dcss blockquote,.dcss ul,.dcss fieldset, .dcss form,.dcss ol,.dcss dl,.dcss dir,.dcss menu { margin: 1.12em 0 }
.dcss h5 { font-size: .83em; margin: 1.5em 0;line-height: 1.5em }
.dcss h6 { font-size: .75em; margin: 1.67em 0;line-height: 1.5em }
.dcss h1, .dcss h2, .dcss h3, .dcss h4, .dcss h5, .dcss h6, .dcss b,.dcss strong { font-weight: bolder }
.dcss blockquote { margin-left: 40px; margin-right: 40px }
.dcss i, .dcss cite, .dcss em,.dcss var,.dcss address { font-style: italic }
.dcss pre, .dcss tt,.dcss  code,.dcss  kbd,.dcss samp { font-family: monospace }
.dcss pre { white-space: pre }
.dcss button,.dcss textarea,.dcss input,.dcss object,.dcss select { display:inline-block; }
.dcss big { font-size: 1.17em }
.dcss small, .dcss sub, .dcss sup { font-size: .83em }
.dcss sub { vertical-align: sub }
.dcss sup { vertical-align: super }
.dcss table { border-spacing: 2px; }
.dcss thead, .dcss tbody,.dcss tfoot { vertical-align: middle }
.dcss td, .dcss th { vertical-align: inherit }
.dcss s, .dcss strike,.dcss  del { text-decoration: line-through }
.dcss hr { border: 1px inset }
.dcss ol, .dcss ul,.dcss  dir, .dcss menu, .dcss dd { margin-left: 40px }
.dcss ol { list-style-type: decimal }
.dcss ol ul,.dcss ul ol, .dcss ul ul, .dcss ol ol { margin-top: 0; margin-bottom: 0 }
.dcss u,.dcss ins { text-decoration: underline }
.dcss br:before { content: "A" }
.dcss :before, .dcss :after { white-space: pre-line }
.dcss center { text-align: center }
.dcss abbr,.dcss acronym { font-variant: small-caps; letter-spacing: 0.1em }
.dcss :link,.dcss :visited { text-decoration: underline }
.dcss :focus { outline: thin dotted invert }
.dcss BDO[DIR="ltr"] { direction: ltr; unicode-bidi: bidi-override }
.dcss BDO[DIR="rtl"] { direction: rtl; unicode-bidi: bidi-override }
.dcss *[DIR="ltr"] { direction: ltr; unicode-bidi: embed }
.dcss *[DIR="rtl"] { direction: rtl; unicode-bidi: embed }
&lt;a href='https://github.com/media' class='user-mention'&gt;@media&lt;/a&gt; print {
    .dcss h1 { page-break-before: always }
    .dcss h1,.dcss h2,.dcss h3,.dcss h4,.dcss h5,.dcss h6 { page-break-after: avoid }
    .dcss ul,.dcss ol,.dcss dl { page-break-before: avoid }
}
</code></pre>

<h2>jQuery操作DOM函数一览</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>自我鉴定</title>
      <link href="/jinguodong-blog/2015/03/self-evaluation/"/>
      <pubDate>2015-03-25T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/03/self-evaluation</guid>
      <content:encoded><![CDATA[<h2>研究生自我鉴定</h2>

<p>本人作为研究生在北京邮电大学网络技术研究院攻读计算机科学与技术专业两年半，毕业之际，回顾研究生期间的学习、工作以及生活，做自我鉴定如下：</p>

<p>本人在思想觉悟上始终对自己有较高的要求，能用科学发展观作为自己的指导思想，正确认识世界、认识社会，能清醒的意识到自己所负担的社会责任，对个人的人生理想和发展目标，有成熟的认识和定位。</p>

<p>在专业课程的学习上，根据自身研究方向的要求，有针对性的认真研读了有关核心课程，为自己的科研工作打下了扎实的基础；并涉猎了一部分其他相关课程，开阔视野，对本研究方向的应用背景以及整个学科的结构有了宏观的认识。</p>

<p>在科研工作上，根据导师的指导，研读了大量论著，逐渐明确了研究方向，通过自身不断的努力，以及与师长同学间的探讨交流，独自主持了一个组合创新型的科研项目，参与了一个企、事业单位委托科研项目，取得了比较满意的成果。在此期间，查阅资料，综合分析等基本素质不断提高，书面表达的能力也得到了锻炼，尤其是独立思考、判断和研究的能力，有了突出进步。</p>

<p>在平时生活中，为人真诚热情，做事认真负责，与同学关系融洽。在工作方面，抱着认真负责的态度和为同学服务的心态，不仅为同学带来方便，也提高了自身组织管理能力和表达能力。在课余时间积极发展自身的业余爱好和兴趣，参加了不少学校活动和社会活动，为个人的全面发展打下了坚实的基础。</p>

<p>毕业在即，在工作实践中，除了提升适应工作要求的具体业务能力，还提高了和同事沟通交流的能力，团队协作的素质也得以培养。在今后的社会生活中，会面临更大的挑战和机遇，我已经做好了准备，迎头而上。我会继续学习，不断进取，勇于开拓，直视困难，校正缺点，实现更高的社会价值。</p>

<h2>班组(基层组织)鉴定：</h2>

<p>该同学在硕士研究生学习阶段，思想上积极要求上进，在思想觉悟上始终严格要求自己，积极向上，不断自我提高。在专业课程的学习上，能根据自身研究方向的要求，有针对性的认真研读了课内外有关书籍，查阅了广泛的专业文献，掌握了本门学科坚实的基础理论知识和系统的专业知识，具有较强的科学研究能力和创新能力，具有很强的实践动手能力。在平时生活中，为人处世和善热情，和同学关系融洽，并积极参与各项集体活动，乐观开朗，乐于助人，意志坚强。希望在以后的工作和学习中，继续保持并发扬优良作风，兢兢业业，争取取得更大的成绩。</p>

<h2>导师对毕业生业务能力、外语水平介绍及对其就业的建议：</h2>

<p>该生在下一代网络和计算机软件方面具有扎实的理论基础和宽广的专业知识，并经常结合实践反思，将理论更好地与实践结合起来，同时具有很强的学习能力和科研能力。学习勤奋上进，善于思考探究，做事认真踏实，为人稳重守信，善于团队合作，具有很好的组织协调能力，并能独立进行一些科研工作。外语水平良好，能娴熟地阅读一些专业相关的外文文献。适合在研究机构从事科研工作或在企业从事软件开发工作。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>研究生毕业</title>
      <link href="/jinguodong-blog/2015/03/graduate-list/"/>
      <pubDate>2015-03-10T00:00:00+08:00</pubDate>
      <author>guodong.j</author>
      <guid>/jinguodong-blog/2015/03/graduate-list</guid>
      <content:encoded><![CDATA[<h2>毕业手续列表</h2>

<ol>
<li>进入“学位”→“毕业学位”→“就业情况信息登记”一栏，填写相关信息并保存；最后进入“学位”→“毕业学位”→“学位授予信息核对”栏，核实各项信息后下载打印《学位授予信息表》并签字。硕士研究生应于所在学院规定的时间内，凭本人签字的《学位授予信息表》到学院教务科领取学位证书。</li>
</ol>


<p>特别提示：</p>

<p>1、就业情况信息登记中：“去向”、“就业单位”、“单位类别”、“工作性质”、“单位所在省市”为必填项。</p>

<p>2、研究生学历电子注册之后方可发放毕业证、派遣证以及讨论学位授予问题。学生本人无需参与学历电子注册流程，只需按照学生所在学院（研究所、中心）的时间要求完成学位论文答辩，并将答辩材料递交至本学院（研究所、中心）即可。后续可关注研究生院网站公布的学历电子注册名单，公布名单后学生可向学生处咨询毕业证书问题。</p>

<p>3、未能参加本次毕业的学生，可于2015年7月毕业。</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
